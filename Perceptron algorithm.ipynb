{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single unit of logic in an artifitial NN\n",
    "# input --> wheights --> sum+step --->output\n",
    "# an algorithm which outputs a binary conclusion (two classes)\n",
    "# a sigle node in a bigger network, meant to mimic a neuron inside a human brain\n",
    "# Ex: \n",
    "# input X = {0.1, 0.5, 0.2}, weights(the impact on data) W =   {0.4, 0.3, 0.6}\n",
    "# sum(X * W) = 0.04, 0.15, 0.12 =  0.31\n",
    "# is this sum = 0.31 > bias (threshold)?\n",
    "# we use an activation function to decide!\n",
    "# --> the step function 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted sum :0.04000000000000001\n",
      "weighted sum :0.19\n",
      "weighted sum :0.31\n",
      "predicted class : 0\n"
     ]
    }
   ],
   "source": [
    "X = [0.1, 0.5, 0.2]\n",
    "W = [0.4, 0.3, 0.6]\n",
    "threshold = 0.5\n",
    "\n",
    "def step(weighted_sum):\n",
    "    if weighted_sum > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def perceptron():\n",
    "    wheighted_sum = 0\n",
    "    for x, w in zip(X, W):\n",
    "        wheighted_sum += x*w\n",
    "        print(f'weighted sum :{wheighted_sum}')\n",
    "    return step(wheighted_sum)\n",
    "\n",
    "output = perceptron()\n",
    "print(f'predicted class : {output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Error (Loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx, w_sum, y, loss\n",
    "# 0, 0.26, 1, 0.58\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss: 0.585026652029182\n",
      "the loss: 0.09691001300805639\n",
      "the loss: 0.3187587626244128\n",
      "the loss: 0.1549019599857432\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#list of w_sum and target class \n",
    "input_data = [(0.26, 1),\n",
    "              (0.20, 0),\n",
    "              (0.48, 1),\n",
    "              (0.30, 0)]\n",
    "\n",
    "def cross_entropy(input_data):\n",
    "    loss = 0\n",
    "    n = len(input_data) # number of examples\n",
    "    for entry in input_data:\n",
    "        w_sum = entry[0]\n",
    "        y = entry[1]\n",
    "        loss += -(y*math.log10(w_sum) + (1-y)*math.log10(1-w_sum))\n",
    "        print(f'the loss: {-(y*math.log10(w_sum) + (1-y)*math.log10(1-w_sum))}')\n",
    "    return loss/n\n",
    "\n",
    "error_term = cross_entropy(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent takes a value between 0 and 1\n",
    "# perceptron provide a value 0 or 1\n",
    "# change the activation function!\n",
    "# include the threshold --> w_sum = X*W+bias (the bias is the threshold)\n",
    "# Ex:\n",
    "# 0.1 * 0.4 + 0.5 * 0.2 + 0.2 * 0.6 + 0.5 = 0.76 \n",
    "# the sigmoid function: 1/(1 + exp(-w_sum)) = 1/(1.46) = 0.68 !!!\n",
    "# --> the loss = 0.49\n",
    "# ===> new_W = W + lr*(target-pred)*x[i]\n",
    "# ===> new_bias = bias + lr*(target-pred)\n",
    "\n",
    "# new_W[0] = 0.4 + 0.1*(0 - 0.68)*0.1 = 0.393\n",
    "# new_W[1] ..\n",
    "# new_W[2] ..\n",
    "\n",
    "\n",
    "# new_bias = 0.5 + 0.1*(0-0.68) = 0.432\n",
    "\n",
    "# a single iteration over the entier datasets of features is an epoch\n",
    "# 4 features = 4 weight updates per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(feature, weights, target, prediction, lr, bias):\n",
    "#     feature, weights : lists of 3 items\n",
    "#     target : integer 0 or 1\n",
    "#     prediction, lr, bias : float poit numbers\n",
    "#     return new_weights, new_bias\n",
    "    new_W = []\n",
    "    new_bias = bias + lr*(target-prediction) \n",
    "    for x, w in zip(feature, weights):\n",
    "        new_w = w + lr*(target - prediction)*x\n",
    "        new_W.append(new_w)\n",
    "    return new_weights, new_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = np.array(([0.1, 0.5, 0.2],\n",
    "                     [0.2, 0.3,0.1],\n",
    "                     [0.7, 0.4, 0.2],\n",
    "                     [0.1, 0.4, 0.3]))\n",
    "targets = np.array([0,1,0,1])\n",
    "weights = np.array([0.4, 0.2, 0.6])\n",
    "bias = 0.5\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train a basic neural network with pandas and numpy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# access machine learning databases with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
